{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCTEL0RMcFalLrPNcOqeT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek0697/Deep-Learning/blob/main/Build%20CNNs%20in%20Numpy/CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpu4mj7Jt50y"
      },
      "source": [
        "# Some Facts about the Convolution Neural Networks\r\n",
        "\r\n",
        "1. In CNNs, an explicit assumption is that the input are images.\r\n",
        "\r\n",
        "2. CNNs are **Translational Invariant**. i.e: If it sees an object, say a flower only at top left corner of images during training, even then it can identify a flower anywhere in the image at test time. MultiLayer Perceptrons lack this feature.\r\n",
        "\r\n",
        "3. **Filters**, also called as Kernels have a small spatial dimension i.e. Height and Width. However, each filter spans through the entire depth of the input volume. for e.g. for input images from CIFAR-10 of size 32x32x3, if we use kernel size of 5x5, the shape of the filters would be 5x5x3 \r\n",
        "\r\n",
        "4. Filters in the same depth slice share weights. This **Parameter Sharing** gives a huge boost to the computation complexity and reduces learnable parameters in a layer.\r\n",
        "\r\n",
        "5. CNNs are not naturally invariant to transformations like scaling or rotation\r\n",
        "\r\n",
        "6. The initial layers learn simple patterns like horizonal/vertical edges, while the deeper layers can learn complex structures like a wheel of a car, bird's beak etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMGXBemUgWFG"
      },
      "source": [
        "# Here is a code for the forward pass for a CNN layer\r\n",
        "\r\n",
        "### A Convolutional Layer accepts an input of shape (B, C, H, W)\r\n",
        "- B: Batch size; A batch of \"B\" number of input samples\r\n",
        "- C: Number of Channels. for eg. N = 3 for an RGB image\r\n",
        "- H: Height of images\r\n",
        "- W: Width of images\r\n",
        "<br>\r\n",
        "\r\n",
        "### Parameters \r\n",
        "- S: Stride\r\n",
        "- P: Padding\r\n",
        "- F: Number of Filters\r\n",
        "- K: Filter size. for eg - 3x3, 5x5\r\n",
        "<br>\r\n",
        "\r\n",
        "### The shape of the output of a layer is computed as\r\n",
        "\r\n",
        "- $Output Width = \\frac{W- F+ 2P}{S}+1$\r\n",
        "\r\n",
        "- $Output Height = \\frac{H- F+ 2P}{S}+1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQzRp2LVgLvf"
      },
      "source": [
        "def forward_pass(x, w, b, stride, pad):\r\n",
        "    \"\"\"\r\n",
        "    Input:\r\n",
        "    1. x: input data of shape (B,C,H,W)\r\n",
        "    2. w: Filter weights of shape (F, C, HH, WW)\r\n",
        "    3. b: biases, of shape (F)\r\n",
        "    \r\n",
        "    Return: out: Output of the forward pass\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    B,C,H,W = np.shape(x)\r\n",
        "    F,C,HH,WW = np.shape(w)\r\n",
        "    \r\n",
        "    '''\r\n",
        "    First we will pad the input x with zeros using np.pad\r\n",
        "    '''\r\n",
        "    Padded_x=np.zeros((B,C, H+(2*pad), W+(2*pad)))\r\n",
        "    \r\n",
        "    for n in range(len(x)):\r\n",
        "        for c in range(len(x[n])):\r\n",
        "            Padded_x[n,c,:,:]=(np.pad(x[n,c], pad, 'constant', constant_values=0))\r\n",
        "\r\n",
        "    \r\n",
        "    '''\r\n",
        "    Calculate the output shape using the formula mentioned before\r\n",
        "    '''\r\n",
        "    output_height= int(((H-HH+(2*pad))/stride)+1)\r\n",
        "    output_width = int(((W-WW+(2*pad))/stride)+1)\r\n",
        "\r\n",
        "    out = np.zeros((B,F,output_height,output_width))\r\n",
        "\r\n",
        "\r\n",
        "    '''\r\n",
        "    Now we will perform Convolution of the padded input with the filters.\r\n",
        "    A rough description of the loop\r\n",
        "\r\n",
        "    for each batch:\r\n",
        "      for each filter:    \r\n",
        "        for sliding across height dimension:\r\n",
        "          for sliding across width dimension:\r\n",
        "          \r\n",
        "            initialize a temp variable that stores the sum to be aggregated across channels\r\n",
        "\r\n",
        "            for each channel:\r\n",
        "              step 1: get the input frame\r\n",
        "              step 2: take the sum(element-wise dot product between the filter and the frame) add it to our temp variable\r\n",
        "\r\n",
        "            Assign the sum to the output \r\n",
        "            \r\n",
        "            move rightwards with incrementing the pointer with the stride for sliding along width dimension\r\n",
        "          move downwards with incrementing the pointer with the stride for sliding along height dimension\r\n",
        "    '''\r\n",
        "\r\n",
        "    for n in range(B):                                                                                  \r\n",
        "        for f in range(F):                                                                             \r\n",
        "            ii=0                                                                                \r\n",
        "            \r\n",
        "            for i in range(output_height):        \r\n",
        "                jj=0\r\n",
        "                for j in range(output_width):\r\n",
        "\r\n",
        "                        sum_conv_frames=0\r\n",
        "                        \r\n",
        "                        for c in range(C):                                                            \r\n",
        "                            \r\n",
        "                            frame=Padded_x[n,c,:,:]\r\n",
        "                            convolution_frame=frame[ii:(ii+HH),jj:(jj+WW)]\r\n",
        "                            sum_conv_frames+= np.sum(np.multiply(convolution_frame,w[f,c,:,:]))\r\n",
        "                        \r\n",
        "                        out[n,f,i,j]=sum_conv_frames+b[f]\r\n",
        "                        jj+=stride\r\n",
        "                \r\n",
        "                ii+=stride                      \r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d92KY-W40fbv"
      },
      "source": [
        "# Some handy tricks while designing a Convolutional Neural Networks\r\n",
        "\r\n",
        "1. The resolution of input can be reduced by setting the 'stride' parameter or using pooling layer after the convolution layer. \r\n",
        "\r\n",
        "2. If we want to keep the resolution of the output same as that of input to the layer, \r\n",
        "- If using 3x3 kernel, use padding of 1 \r\n",
        "- If using 5x5 kernel, use padding of 2 \r\n",
        "- If using 7x7 kernel, use padding of 3  and so on....\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6X79uevTNtf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}